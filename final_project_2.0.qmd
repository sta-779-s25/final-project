---
title: "Final Project"
author: "Hugh Ford and Lauren Chandler-Holtz"
format:
  pdf:
    fontsize: 12pt
editor: visual
execute: 
  echo: false
  message: false
  warning: false
abstract:
  This is an abstract
---

```{r}
library(causaldata)
library(dagitty)
library(ggdag)
library(tidyverse)
library(knitr)
library(naniar)
library(gtsummary)
library(gt)
library(broom)
library(readxl)
library(propensity)
library(halfmoon)
library(patchwork)
library(visdat)
library(survey)
library(labelled)
library(tipr)
library(kableExtra)

data("castle")
```

# Causal Question

Using data assembled by Cheng Cheng and Mark Hoekstra for their study, "Does Strengthening Self-Defense Law Deter Crime or Escalate Violence? Evidence from Expansions to Castle Doctrine" (2013), primarily sourced from the FBI Uniform Crime Reports Summary, we sought to examine the causal relationship between implementing Castle Doctrine and murder rates in the U.S. Focusing on the “treated” population—states that passed Castle Doctrine laws between 2001 and 2010—we investigated whether these states should repeal Castle Doctrine laws in order to bring down murder rates. Formally, our causal question was: among the states that passed Castle Doctrine laws between 2001 and 2010, what was the effect of that implementation on the state’s murder rate?

# Background

As Cheng and Hoekstra explain in their work, “Castle Doctrine” stems from English Common Law, allowing an exception to the “duty to retreat” when an individual is in their own home. In the United States, the legal history of Castle Doctrine dates back to the 1700s as a legacy of English colonial rule. During the era of Westward Expansion, the concept broadened to cover not only the home but also surrounding property and, in many cases, any place one had a legal right to be. Castle Doctrine principles were formally codified in 1985, when Colorado passed the “Make My Day” law, which removed any civil or criminal liability for the use of force, including lethal force, against a home invader.

The modern expansion of self-defense laws began with Florida in 2005, when it became the first state to strengthen its self-defense protections and explicitly expand Castle Doctrine to places outside the home. This change in statute was quickly adopted by about twenty other states in the following years. Since 2005, Castle Doctrine has spread to the significant majority of US states. However, laws vary state to state, with some being limited to the home while others are expanded to include other places, e.g., one’s place of work or in one’s vehicle.

Castle Doctrine often goes hand-in-hand with so-called "Stand Your Ground" laws; however, Castle Doctrine is distinguished from such laws in that it specifically applies to the home (and sometimes other areas such as one’s workplace or car) and allows the use of deadly force, even if disproportionate. In contrast, Stand Your Ground typically applies to anywhere one has a legal right to be but only allows defensive proportional force.

Proponents of Castle Doctrine argue that it is a useful deterrent to crime and contributes to public safety. Critics, on the other hand, contend that these laws may instead unnecessarily escalate levels of violence. The aim of our research is to investigate whether Castle Doctrine laws have a significant impact on states’ murder rates. For the purpose of our research, "murder rate" excludes cases that were ruled justifiable homicides.

# Creating a Directed Acyclic Graph (DAG)

We began by creating a DAG to represent the causal relationships between variables in our dataset. First, we specified our outcome as the state’s murder rate per 100,000 citizens and our exposure as passing a “Castle Doctrine” law between 2001 and 2010, as designated by Cheng and Hoekstra. We then identified several covariates we thought would play important causal roles in relation to murder rate and whether a state would pass a Castle Doctrine law.

For example, we believed that the murder rate as well as the decision to pass a Castle Doctrine Law could depend on the state’s demographics as well as the prevalence of certain crimes in the year prior, such as robbery with a firearm, assault, and even murder itself, as these crimes would likely create an atmosphere of fear that would sway voters and politicians to pass such a law. In our analysis, we lagged all the crime variables (assault, burglary, motor vehicle theft, murder, robbery, and robbery with a firearm) by one year, so we were able to accurately capture their causal effect on the next year.

Our full DAG illustrating the causal relationships we believe are in effect is presented **Figure 1**.

```{r, echo = FALSE, warning = FALSE, message = FALSE}
castle$treat_year <- ifelse(castle$post == 1, castle$year, 0)

lower <- 1
upper <- 11
i <- 1
while(i < 51){
  treat_year_1 <- min(castle$treat_year[lower:upper][castle$treat_year[lower:upper] != 0])
  castle$treat_year[lower:upper] <- rep(treat_year_1, 11)
  lower <- upper + 1
  upper <- lower + 10
  i <- i + 1
}

castle <- castle |>
  mutate(years_after_treat = year - treat_year)

castle$years_after_treat <- ifelse(castle$years_after_treat == -Inf, NA, castle$years_after_treat)


castle <- castle |>
  group_by(sid) |>
  mutate_at(c("assault", "burglary", "homicide", "larceny", "motor", "l_larceny", "l_motor", 
              "l_police", "l_income", "l_exp_subsidy", "l_exp_pubwelfare"), lag) |>
  mutate(murder_lag = lag(murder)) |> 
  ungroup()


castle$sid <- ifelse(as.numeric(castle$sid) > 8, castle$sid - 1, castle$sid)

state_id_list <- read_excel("state_id_list_fixed.xlsx", col_names = FALSE)
colnames(state_id_list) <- c("state", "pop", "sid")
state_id_ranks <- state_id_list |>
  select(state, sid)
castle_dat <- full_join(castle, state_id_list, by = "sid")

castle_dat <- castle_dat[castle_dat$year != 2000,]

castle_for_tab <- castle_dat |> 
  select(!(starts_with("r20") |
             starts_with("trend") |
             starts_with("lead") |
             starts_with("lag") |
             starts_with("years"))) 

#CCA + Dropping Washington
castle_dat <- castle_dat |> 
  select(!(starts_with("r20") |
             starts_with("trend") |
             starts_with("lead") |
             starts_with("lag"))) |>
  drop_na(robbery_gun_r) |>
  filter(sid != 47)
```

```{r}
# young_male_race is taking place of blackm_15_24 + whitem_15_24 + blackm_25_44 + whitem_25_44

castle_dag <- dagify(
  murder ~ young_male_race + poverty + popwt + robbery_gun_r + l_police + post + years_after_treat + robbery,
  
  post ~ homicide + robbery_gun_r + assault + burglary + motor + murder_lag + robbery,
  
  burglary ~ poverty + young_male_race,
  homicide ~ poverty + young_male_race,
  motor ~ poverty + young_male_race,
  robbery ~ poverty + young_male_race,
  assault ~ poverty + young_male_race,
  
  poverty ~ unemployrt + l_exp_subsidy + l_exp_pubwelfare + l_income,
  
  l_police ~ l_income,
  
  outcome = "murder",
  exposure = "post",
  labels = c(
    murder = "Murder",
    murder_lag = "Murder Lagged",
    unemployrt = "Unemployment",
    young_male_race = "Male Demo.",
    poverty = "Poverty",
    popwt = "Population",
    robbery_gun_r = "Robbery w. Firearm",
    l_exp_subsidy = "Subsidy",
    l_exp_pubwelfare = "Welfare",
    l_police = "Police",
    post = "Castle Doctrine",
    years_after_treat = "Years Post Castle",
    
    homicide = "Homicide",
    robbery = "Robbery",
    assault = "Assault",
    burglary = "Burglary",
    motor = "Auto Theft",
    l_income = "Income")
)
```

```{r, fig.height=10, fig.width=10, fig.cap = "DAG"}
ggdag(castle_dag, layout = "nicely", use_labels = "label", text = FALSE) +
  labs(caption = "DAG") +
  theme_dag()
```

# Adjustments and Missing Data

## Adjustment Sets

After mapping out our DAG, the next step in our Causal Analysis was identifying potential confounders that we would need to adjust for in our model. There are two possible adjustment sets for our DAG as shown in **Figure 2**. The minimal adjustment set includes homicide, assault, burglary, robbery, robbery involving a firearm, and motor vehicle theft, while the second is made up of robbery, poverty, robbery involving a firearm, and all four male age-race variables (% of Black males age 15-24, % of white males age 15-24, % of black males age 25-44, % of white males age 25-44).

```{r, fig.height=9, fig.width=10, fig.cap = "DAG w. Adjustment Sets"}
ggdag_adjustment_set(castle_dag, text_col = "black",
                     use_labels = "label",
                     text = FALSE) +
  theme_dag()
```

## Missing Data

In our analysis of missing data, we found that robbery involving a firearm has about 1.09% missing data (6 observations), and is only missing for one state, Washington. No other variables include missing data. Due to the low level of missingness and the fact that Washington did not implement Castle Doctrine, which is the much larger subpopulation, we proceeded with a complete case analysis and excluded the state of Washington from consideration.

```{r, fig.cap = "Missing Data Visualization"}
vis_dat(castle_for_tab)
```



# Propensity Weighting

We used inverse propensity score weighting (IPW) in our analysis, which allowed us simulate what the relationship between exposure (implementing Castle Doctrine) and outcome (murder rates) would have looked like if our data had come from a randomized trial instead of being observational. Since we targeted the effect of the policy on the treated states, we employed the inverse probability weight for the Average Treatment Effect Among the Treated (ATT) to estimate the effect. Here, the propensity score for each observation is the probability of the state implementing Castle Doctrine given their particular covariate values. To simplify our model and minimize potential measurement error, we used the minimal adjustment set to identify confounders. We then fit a logistic regression to calculate the probabilities of states passing Castle Doctrine with the model shown below:

$log\_odds_i = S_{1:3}(Homicide_i) + S_{1:2}(Burglary_i) + Assault_i + MotorTheft_i + Robbery_i + ArmedRobbery_i$

## Unweighted Data

```{r}
castle_select <- castle_dat |>
  select(c(post, assault, burglary, homicide, motor, robbery, robbery_gun_r)) |> 
  mutate(post = ifelse(post == 0, "Pre-Doctrine", "Post-Doctrine")) |> 
  set_variable_labels(
    post = "Passage of Castle Doctrine",
    assault = "Assault",
    burglary = "Burglary",
    homicide = "Homicide",
    motor = "Motor Vehicle Theft",
    robbery = "Robbery",
    robbery_gun_r = "Robbery w. Firearm"
  )

tbl_summary(
  castle_select,
  by = post
) |>
  add_overall(last = TRUE) |> 
  modify_caption("**Sample Characteristics by Castle Doctrine**") |> 
  modify_footnote(everything() ~ "Median (Q1, Q3); Rates per 100,000 persons") |>
  as_kable_extra(format = "latex") |>
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    full_width = FALSE,
    position = "center"
  )
```

Before weighting our subpopulations with IPW, we observe substantial imbalances between the pre and post-Doctrine subpopulations. From **Table 1**, we see the Pre-Doctrine group had 411 observations, with lower median crime rates across all covariates in the adjustment set. This imbalance is also reflected in the distributions of propensity scores, displayed in Figure. The distribution of propensity scores for states that had not passed Castle Doctrine laws is highly right skewed, with a majority of the scores falling between 0.0 and 0.2, while the distribution for post-Castle Doctrine states is spread relatively uniformly from 0.0 to 0.7.

```{r}
propensity_model <- glm(post ~ splines::ns(homicide, 3) +
                          splines::ns(burglary, 2) +
                          assault + motor + robbery +
                          robbery_gun_r ,
                        data = castle_dat,
                        family = "binomial")
  
castle_dat <- propensity_model |>
  augment(type.predict = "response", data = castle_dat) |>
  mutate(w_att = wt_att(.fitted, post, exposure_type = "binary"))
```

```{r, fig.cap = "Mirrored Histogram, Unweighted"}
ggplot(castle_dat, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post))) +
  labs(x = "Propensity Score", fill = "Passed Castle Doctrine", caption = "Unweighted") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

## ATT Weighting

```{r, warning = FALSE}

castle_select2 <- castle_dat |>
  select(c(post, assault, burglary, homicide, motor, robbery, robbery_gun_r, w_att)) |> 
  mutate(post = ifelse(post == 0, "Pre-Doctrine", "Post-Doctrine")) |> 
    set_variable_labels(
    post = "Passage of Castle Doctrine",
    assault = "Assault",
    burglary = "Burglary",
    homicide = "Homicide",
    motor = "Motor Vehicle Theft",
    robbery = "Robbery",
    robbery_gun_r = "Armed Robbery"
  )

svy_des <- svydesign(
  ids = ~1,
  data = castle_select2,
  weights = ~w_att
)

hdr <- paste0(
  "**{level}**  \n",
  "N = {n_unweighted}; ESS = {format(n, digits = 1, nsmall = 1)}"
)

tbl_svysummary(svy_des,
               by = post,
               include = c(assault, burglary, homicide, motor, robbery, robbery_gun_r))|> 
    add_overall(last = TRUE) |> 
  add_ess_header(header = hdr) |> 
  modify_caption("**Sample Characteristics by Re-Weighted Castle Doctrine**") |> 
  modify_footnote(everything() ~ "Median (Q1, Q3); Rates per 100,000 persons")|>
  as_kable_extra(format = "latex") |>
  kable_styling(
  latex_options = c("scale_down", "hold_position"),
  full_width = FALSE,
  position = "center"
  )

```

```{r, fig.cap = "Mirrored Histogram, Unweighted and Weighted"}
ggplot(castle_dat, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = .6, fill = "grey") +
  labs(x = "Propensity Score", y = "Count") +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post), weight = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Propensity Score", y = "Count", fill = "Passed Castle Doctrine") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")

```

```{r, fig.cap = "Mirrored Histogram, ATT Only"}
ggplot(castle_dat, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post), weight = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Propensity Score", y = "Count", fill = "Passed Castle Doctrine") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

To account for the differences across the two subpopulations, we fit the previously-specified propensity score model, predicting a state's probability of passing a Castle Doctrine law based on the covariates in our adjustment set. After reweighting our data, we observe in **Table 2** that the median value of the covariates are much more matched between between Castle Doctrine states and non-Castle Doctrine states.

We display the distributions of the propensity scores for the subpopulations in the mirrored histograms in **Figures 5** and **6**. The distributions of propensity scores in the mirrored histograms demonstrate substantially improved balance between the two groups, and we do not observe any alarming positivity issues. 

We also used a Love Plot (**Figure 7**) to visualize the effect of ATT weighting on the Standardized Mean Difference for each confounding covariate. Before weighting, we observe the SMD for each covariate except for Robbery was relatively large, reflecting the unbalance between Castle Doctrine states and non-Castle Doctrine states across these variables. After re-weighting, we see that the SMDs across the covariates are below the 0.1 threshold, suggesting that the weighting method has balanced the two pseudo-populations across each covariate very effectively. These results align with the conclusions we were able to draw from comparing the unweighted and re-weighted tables.

```{r, fig.cap = "Love Plot"}
dat_for_love <- castle_dat %>%
  select(assault, burglary, homicide, motor, robbery, robbery_gun_r, post, w_att)

colnames(dat_for_love) <- c("Assault", "Burglary", "Homicide", "Motor Vehicle Theft", "Robbery", "Armed Robbery \n with Gun Rate", "post", "w_att")

weighted_for_love <- tidy_smd(
  dat_for_love,
  .vars = c(Assault, Burglary, Homicide, `Motor Vehicle Theft`, Robbery, `Armed Robbery \n with Gun Rate`),
  .group = post,
  .wts = c(w_att)
)

ggplot(data = weighted_for_love, aes(x = abs(smd), y = variable, group = method, color = method)) +
  geom_love() +
  scale_color_manual(values = c("coral", "turquoise"), labels = c("Observed", "ATT Weighted")) + 
  labs(color = "Method", x = "Absolute Value of SMD", y = "Variable") +
  theme_minimal()
```

Finally, we compared the empirical cumulative distribution functions (eCDFs) of our confounders. Since all confounders in the adjustment set were continuous, we constructed eCDF plots for each one. Although we were not able to achieve perfect correspondence between the two subpopulations through weighting alone, incorporating second and third degree splines (as reflected in our propensity score model) on Homicide and Burglary helped to bring the eCDFs for those covariates closer together (see Appendix). Overall, our checks suggested that the necessary assumptions for causal inference appeared to be reasonably met, so we proceeded with effect estimation.

# G-Computation

To capture the time-varying effect of the implementation of Castle Doctrine, we performed G-Computation using a linear model that incorporated ATT weights from our IPW model, along with an interaction term representing the number of years before and after enactment of a state's Castle Doctrine law:\

$Murder_i = CastleDoctrine_i + Years_i + CastleDoctrine_i:Years_i$ \

We simulated counterfactual data for each of the four years before and after the enactment of Castle Doctrine laws. For each year, we computed the difference in murder rates between the post- and pre-enactment periods for both Castle Doctrine and non–Castle Doctrine states. Using these differences, we conducted a pseudo–difference-in-differences analysis to estimate the treatment effect of Castle Doctrine over time. Based on our model, we estimated the average treatment effect among the treated (ATT) for four time intervals surrounding the law's passage. Four-years was the largest symmetric time-span around Castle Doctrine passage available for comparison in our data. The point estimates of these effects are presented in **Table 3**. Across all periods, we observed a negative effect on the murder rate; however, to assess the reliability of these estimates, we conducted an uncertainty analysis

```{r}
standardized_model <- lm(murder ~ post*years_after_treat, data = castle_dat, weights = w_att)

years_pre_post <- c(1:4)

estimate_df <- data.frame("years_pre_post" = years_pre_post,
                           "ATT_est" = rep(NA, 4))

for (j in years_pre_post){
    
    standardized_model <- lm(murder ~ post*years_after_treat,
                        data = castle_dat, weights = w_att)
    
    castle_dat_minus_j_yes <- castle_dat |>
      mutate(years_after_treat = -1*j, post = 1)
  
    castle_dat_minus_j_no <- castle_dat |>
      mutate(years_after_treat = -1*j, post = 0)
    
    castle_dat_plus_j_yes <- castle_dat |>
      mutate(years_after_treat = j, post = 1)
    
    castle_dat_plus_j_no <- castle_dat |>
      mutate(years_after_treat = j, post = 0)
    
    new_data_plus_j_yes <- standardized_model |>
      augment(newdata = castle_dat_plus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_plus_j_no <- standardized_model |>
      augment(newdata = castle_dat_plus_j_no) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_yes <- standardized_model |>
      augment(newdata = castle_dat_minus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_no <- standardized_model |>
      augment(newdata = castle_dat_minus_j_no) |>
      rename(murder_est = .fitted)
    
    estimate_df$years_pre_post[j] <- j
    
    estimate_df$ATT_est[j] <- (mean(new_data_plus_j_yes$murder_est) -
                                 mean(new_data_minus_j_yes$murder_est)) -
                              (mean(new_data_plus_j_no$murder_est) -
                                 mean(new_data_minus_j_no$murder_est))
}

estimate_df$ATT_est <- round(estimate_df$ATT_est, 3)

colnames(estimate_df) <- c("Years Before/\\\\After Castle Doctrine", "ATT Point Est.")


estimate_df |> 
  kbl(format = "latex", booktabs = TRUE, escape = F,
      caption = "Estimated ATT by Year Relative to Castle Doctrine Ratification") |> 
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    full_width = FALSE,
    position = "center"
  )
```

## Measures of Uncertainty

To assess the uncertainty of our estimates, we employed a bootstrap approach, resampling the data 1,000 times to generate a distribution of estimated treatment effects. From this empirical distribution, we calculated the $2.5$ and $97.5$ percentiles to construct 95\% confidence intervals for each point estimate. All four confidence intervals contained zero, indicating that none of the estimated effects are statistically significant at the 95\% level.

```{r}
set.seed(779)

n_bootstrap <- 1000

bootstrap_df <- data.frame("years_pre_post" = years_pre_post,
                           "mean_ATT" = rep(NA, 4),
                           "sd_ATT" = rep(NA, 4),
                           "ci.l" = rep(NA, 4),
                           "ci.u" = rep(NA, 4))

for (j in years_pre_post){
  
  bootstrap_est <- vector(length = n_bootstrap)
  
  for (b in 1:n_bootstrap){
    boot_dat <- castle_dat |> 
      slice_sample(n = nrow(castle_dat), replace = T)
    
    standardized_boot <- lm(murder ~ post*years_after_treat,
                        data = boot_dat, weights = w_att)
    
    boot_dat_minus_j_yes <- boot_dat |>
      mutate(years_after_treat = -1*j, post = 1)
  
    boot_dat_minus_j_no <- boot_dat |>
      mutate(years_after_treat = -1*j, post = 0)
    
    boot_dat_plus_j_yes <- boot_dat |>
      mutate(years_after_treat = j, post = 1)
    
    boot_dat_plus_j_no <- boot_dat |>
      mutate(years_after_treat = j, post = 0)
    
    new_data_plus_j_yes <- standardized_boot |>
      augment(newdata = boot_dat_plus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_plus_j_no <- standardized_boot |>
      augment(newdata = boot_dat_plus_j_no) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_yes <- standardized_boot |>
      augment(newdata = boot_dat_minus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_no <- standardized_boot |>
      augment(newdata = boot_dat_minus_j_no) |>
      rename(murder_est = .fitted)
    
    bootstrap_est[b] <- (mean(new_data_plus_j_yes$murder_est) -
                           mean(new_data_minus_j_yes$murder_est)) -  
      (mean(new_data_plus_j_no$murder_est) - mean(new_data_minus_j_no$murder_est))
    
  }
  
  bootstrap_df$mean_ATT[j] <- round(mean(bootstrap_est),3)
  
  bootstrap_df$sd_ATT[j] <- round(sd(bootstrap_est),3)
  
  bootstrap_df$ci.l[j] <- round(quantile(bootstrap_est, 0.025),3)
  
  bootstrap_df$ci.u[j] <- round(quantile(bootstrap_est, 0.975),3)
}

colnames(bootstrap_df) <- c("Years Before/\\\\After Castle Doctrine", "ATT Estimate", "ATT Standard Dev.", "95\\% CI Lower Bound", "Upper Bound")

bootstrap_df |> 
  kbl(format = "latex", booktabs = TRUE, escape = FALSE) |> 
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    full_width = FALSE,
    position = "center"
  )


```

# Sensitivity Analysis:

## Alternate Adjustment Set:

First, we do an alternate DAG analysis. If our variables are well-measured and our DAG is correct, then we would expect each different adjustment set to result in very similar effects (each adjustment set should produce an unbiased estimate of the true causal effect). However, we see that the effects for the alternative adjustment set are notably smaller than those for the initial adjustment set—the effect direction is the same, but, especially the further out from implementation we go, the smaller the effect compared to the original DAG. This discrepancy strongly suggests that the causal structure specified in our DAG is incorrect; we made some alterations to the DAG, such as connecting the police presence variable to crime variables, but there was little to no change in the results.

```{r}
propensity_model_alt <- glm(post ~ splines::ns(poverty, 5) + robbery + splines::ns(robbery_gun_r, 4) + whitem_15_24 + blackm_15_24 + whitem_25_44 + blackm_25_44, data = castle_dat, family = "binomial")

castle_dat_alt <- propensity_model_alt |>
  augment(type.predict = "response", data = castle_dat) |>
  mutate(w_att = wt_att(.fitted, post, exposure_type = "binary"))
```

```{r}
years_pre_post <- c(1:4)

estimate_df <- data.frame("years_pre_post" = years_pre_post,
                           "ATT_est" = rep(NA, 4))

for (j in years_pre_post){
    
    standardized_model <- lm(murder ~ post*years_after_treat,
                        data = castle_dat_alt, weights = w_att)
    
    castle_dat_minus_j_yes <- castle_dat_alt |>
      mutate(years_after_treat = -1*j, post = 1)
  
    castle_dat_minus_j_no <- castle_dat_alt |>
      mutate(years_after_treat = -1*j, post = 0)
    
    castle_dat_plus_j_yes <- castle_dat_alt |>
      mutate(years_after_treat = j, post = 1)
    
    castle_dat_plus_j_no <- castle_dat_alt |>
      mutate(years_after_treat = j, post = 0)
    
    new_data_plus_j_yes <- standardized_model |>
      augment(newdata = castle_dat_plus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_plus_j_no <- standardized_model |>
      augment(newdata = castle_dat_plus_j_no) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_yes <- standardized_model |>
      augment(newdata = castle_dat_minus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_no <- standardized_model |>
      augment(newdata = castle_dat_minus_j_no) |>
      rename(murder_est = .fitted)
    
    estimate_df$years_pre_post[j] <- j
    
    estimate_df$ATT_est[j] <- (mean(new_data_plus_j_yes$murder_est) -
                                 mean(new_data_minus_j_yes$murder_est)) -
                              (mean(new_data_plus_j_no$murder_est) -
                                 mean(new_data_minus_j_no$murder_est))
}

estimate_df$ATT_est <- round(estimate_df$ATT_est, 3)

colnames(estimate_df) <- c("Years Before/\\\\After Castle Doctrine", "ATT Point Est.")

estimate_df |> 
  kbl(format = "latex", booktabs = TRUE, escape = F) |> 
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    full_width = FALSE,
    position = "center"
  )
```

```{r}
set.seed(779)

n_bootstrap <- 1000

bootstrap_df <- data.frame("years_pre_post" = years_pre_post,
                           "mean_ATT" = rep(NA, 4),
                           "sd_ATT" = rep(NA, 4),
                           "ci.l" = rep(NA, 4),
                           "ci.u" = rep(NA, 4))

bootstrap_df_names_clean <- bootstrap_df

for (j in years_pre_post){
  
  bootstrap_est <- vector(length = n_bootstrap)
  
  for (b in 1:n_bootstrap){
    boot_dat <- castle_dat_alt |> 
      slice_sample(n = nrow(castle_dat_alt), replace = T)
    
    standardized_boot <- lm(murder ~ post*years_after_treat,
                        data = boot_dat, weights = w_att)
    
    boot_dat_minus_j_yes <- boot_dat |>
      mutate(years_after_treat = -1*j, post = 1)
  
    boot_dat_minus_j_no <- boot_dat |>
      mutate(years_after_treat = -1*j, post = 0)
    
    boot_dat_plus_j_yes <- boot_dat |>
      mutate(years_after_treat = j, post = 1)
    
    boot_dat_plus_j_no <- boot_dat |>
      mutate(years_after_treat = j, post = 0)
    
    new_data_plus_j_yes <- standardized_boot |>
      augment(newdata = boot_dat_plus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_plus_j_no <- standardized_boot |>
      augment(newdata = boot_dat_plus_j_no) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_yes <- standardized_boot |>
      augment(newdata = boot_dat_minus_j_yes) |>
      rename(murder_est = .fitted)
    
    new_data_minus_j_no <- standardized_boot |>
      augment(newdata = boot_dat_minus_j_no) |>
      rename(murder_est = .fitted)
    
    bootstrap_est[b] <- (mean(new_data_plus_j_yes$murder_est) -
                           mean(new_data_minus_j_yes$murder_est)) -  
      (mean(new_data_plus_j_no$murder_est) - mean(new_data_minus_j_no$murder_est))
    
  }
  
  ## Rounded
  bootstrap_df_names_clean$mean_ATT[j] <- round(mean(bootstrap_est),3)
  
  bootstrap_df_names_clean$sd_ATT[j] <- round(sd(bootstrap_est),3)
  
  bootstrap_df_names_clean$ci.l[j] <- round(quantile(bootstrap_est, 0.025),3)
  
  bootstrap_df_names_clean$ci.u[j] <- round(quantile(bootstrap_est, 0.975),3)
  
  ## Unrounded
  bootstrap_df$mean_ATT[j] <- mean(bootstrap_est)
  
  bootstrap_df$sd_ATT[j] <- sd(bootstrap_est)
  
  bootstrap_df$ci.l[j] <- quantile(bootstrap_est, 0.025)
  
  bootstrap_df$ci.u[j] <- quantile(bootstrap_est, 0.975)
}

colnames(bootstrap_df_names_clean) <- c("Years Before/\\\\After Castle Doctrine", "ATT Estimate", "ATT Standard Dev.", "95\\% CI Lower Bound", "Upper Bound")

bootstrap_df_names_clean |> 
  kbl(format = "latex", booktabs = TRUE, escape = FALSE) |> 
  kable_styling(
    latex_options = c("scale_down", "hold_position"),
    full_width = FALSE,
    position = "center"
  )

```

Our confidence intervals still include zero and thus the effects are once again not significant.

## Tipping Point Analysis

Finally, we assessed the potential impact of an unmeasured confounder on the relationship between Castle Doctrine and Murder Rate. The graph below illustrates how varying strength of an unmeasured confounder's associations with the exposure (Castle Doctrine) and the outcome (Murder Rate) would bias the estimated effect. Based on the graphs, we can identify tipping points, where the adjusted effect crosses the null.

For example, if a one standard deviation change in the unmeasured confounder reduced murder rate by 1 percent and was associated with a probability of 0.35 of not passing a Castle Doctrine law (i.e. -0.35 on the x-axis), then the upper bound of adjusted effect would cross zero, such that the 95% CI for the ATT is entirely below zero, representing a significant negative effect. Since our dataset is limited in its scope of variables, it is reasonable to believe that such a confounder might exist.

Finally, we assessed the potential impact of an unmeasured confounder on the causal relationship using a tipping point analysis. Because our effect at 1 through 4 years from implementation was not significant at a 95% significance level, our tipping point focused on the confounder needed to make the effect significant in either a positive or negative direction. For conciseness, we focused on the effect at 1 year before and after implementation. The graphs below illustrate how varying strength of an unmeasured confounder's associations with the exposure (Castle Doctrine) and the outcome (Murder Rates) would bias the estimated effect at one year before and after exposure. Based on the graph, we can identify tipping points, where the adjusted effect crosses the null.

```{r, fig.caption = "Unmeasured Confounder Plot for Significant Negative Effect -1"}
#Unmeasured confounder to make significant negative effect - 1 year pre/post
ci.u <- bootstrap_df[1,"ci.u"]

adjust_df <- adjust_coef(
  effect_observed = ci.u,
  exposure_confounder_effect = rep(seq(0, -1, by = -0.05), each = 7),
  confounder_outcome_effect = rep(seq(-1, -7, by = -1), times = 21),
  verbose = FALSE
)

ggplot(
  adjust_df,
  aes(
    x = exposure_confounder_effect,
    y = effect_adjusted,
    group = confounder_outcome_effect
  )
) +
  geom_hline(yintercept = ci.u, lty = 2) +
  geom_hline(yintercept = 0, lty = 3, color = "red", lwd = 1) +
  geom_point() +
  geom_line() +
  geom_label(
    data = adjust_df[141:147, ],
    aes(
      x = exposure_confounder_effect,
      y = effect_adjusted,
      label = confounder_outcome_effect
    )
  ) +
  labs(
    x = "Exposure - unmeasured confounder effect",
    y = "Adjusted Effect",
    title = "Effect of Unmeasured Confounder on Treatment Effect"
  )


```

Looking at the negative plot (shown above), we observed that, for example, if a one standard deviation change in the unmeasured confounder reduced murder rate by 2 percent and was associated with a probability of 0.45 of not passing a Castle Doctrine law (i.e. -0.45 on the x-axis), then the upper bound of the adjusted effect would cross zero, such that the 95% CI for the ATT is entirely below zero, representing a significant negative effect. Since our dataset is limited in its scope of variables, it is reasonable to believe that such a confounder might exist.

```{r, fig.caption = "Unmeasured Confounder Plot for Significant Positive Effect -1"}
#Unmeasured confounder to make significant positive effect - 1 year pre/post
ci.l <- bootstrap_df[1,"ci.l"]

adjust_df <- adjust_coef(
  effect_observed = ci.l,
  exposure_confounder_effect = rep(seq(0, -1, by = -0.05), each = 7),
  confounder_outcome_effect = rep(seq(1, 7, by = 1), times = 21),
  verbose = FALSE
)

ggplot(
  adjust_df,
  aes(
    x = exposure_confounder_effect,
    y = effect_adjusted,
    group = confounder_outcome_effect
  )
) +
  geom_hline(yintercept = ci.l, lty = 2) +
  geom_hline(yintercept = 0, lty = 3, color = "red", lwd = 1) +
  geom_point() +
  geom_line() +
  geom_label(
    data = adjust_df[141:147, ],
    aes(
      x = exposure_confounder_effect,
      y = effect_adjusted,
      label = confounder_outcome_effect
    )
  ) +
  labs(
    x = "Exposure - unmeasured confounder effect",
    y = "Adjusted Effect",
    title = "Effect of Unmeasured Confounder on Treatment Effect"
  )
```

Looking at the positive plot (shown above), we observe that, if a one standard deviation change in the unmeasured confounder increased murder rate by 3 percent and was associated with a probability of 0.67 of not passing a Castle Doctrine law (i.e. -0.67 on the x-axis), then the lower bound of the adjusted effect would cross zero, such that the 95% CI for the ATT is entirely positive, representing a significant positive effect on the murder rate. Such a confounder is somewhat less likely than the previous; however, again, given the limitations in our data, more investigation would be needed to completely rule out such a confounder.

# References

Cheng, Cheng, and Mark Hoekstra. 2013. “Does Strengthening Self-Defense Law Deter Crime or Escalate Violence? Evidence from Expansions to Castle Doctrine.” Journal of Human Resources 48 (3): 821–54.

Huntington-Klein, N., & Barrett, M. (2024, October 24). Castle Dataset. R PACKAGES. https://r-packages.io/datasets/castle 

```{r ref.label = knitr::all_labels()}
#| echo: true
#| eval: false
```

# Appendix: {.appendix}

```{r}
miss_tab <- miss_var_summary(castle_for_tab)
colnames(miss_tab) <- c("Variable", "Missing (n)", "Percent Missing")

miss_tab |>
  gt()
```

## Model before the splines: {.appendix}

### Mirrored Histogram: {.appendix}

```{r, fig.caption = "Mirrored Histograms of Propensity Scores No Splines"}
propensity_model_test <- glm(post ~ homicide + burglary + assault + motor + robbery + robbery_gun_r , data = castle_dat, family = "binomial")

castle_dat_test <- propensity_model_test |>
  augment(type.predict = "response", data = castle_dat) |>
  mutate(w_att = wt_att(.fitted, post, exposure_type = "binary"))

ggplot(castle_dat_test, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = .6, fill = "grey") +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post), weight = w_att)) +
  labs(x = "Propensity Score", fill = "Passed Castle Doctrine", caption = "ATT", y = "Count") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")

ggplot(castle_dat_test, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post), weight = w_att)) +
  labs(x = "Propensity Score", fill = "Passed Castle Doctrine", caption = "ATT", y = "Count") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")
```

### Love Plot: {.appendix}

```{r}
dat_for_love <- castle_dat_test %>%
  select(assault, burglary, homicide, motor, robbery, robbery_gun_r, post, w_att)

colnames(dat_for_love) <- c("Assault", "Burglary", "Homicide", "Motor Vehicle Theft", "Robbery", "Armed Robbery \n with Gun Rate", "post", "w_att")
weighted_for_love <- tidy_smd(
  castle_dat_test,
  .vars = c(assault, burglary, homicide, motor, robbery, robbery_gun_r),
  .group = post,
  .wts = c(w_att)
)

ggplot(data = weighted_for_love, aes(x = abs(smd), y = variable, group = method, color = method)) +
  geom_love() +
  theme_minimal()
```

### eCDFs: {.appendix}

```{r}
p1 <- ggplot(castle_dat_test, aes(x = burglary, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Burglary", color = "Castle Implemented") +
  theme_minimal()

p2 <- ggplot(castle_dat_test, aes(x = burglary, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Burglary", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p3 <- ggplot(castle_dat_test, aes(x = assault, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Assault", color = "Castle Implemented") +
  theme_minimal()

p4 <- ggplot(castle_dat_test, aes(x = assault, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Assault", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p5 <- ggplot(castle_dat_test, aes(x = robbery, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Robbery", color = "Castle Implemented") +
  theme_minimal()

p6 <- ggplot(castle_dat_test, aes(x = robbery, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Robbery", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p7 <- ggplot(castle_dat_test, aes(x = homicide, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Homicide", color = "Castle Implemented") +
  theme_minimal()

p8 <- ggplot(castle_dat_test, aes(x = homicide, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Homicide", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p9 <- ggplot(castle_dat_test, aes(x = robbery_gun_r, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Robbery with Gun", color = "Castle Implemented") +
  theme_minimal()

p10 <- ggplot(castle_dat_test, aes(x = robbery_gun_r, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Robbery with Gun", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p11 <- ggplot(castle_dat_test, aes(x = motor, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Motor", color = "Castle Implemented") +
  theme_minimal()

p12 <- ggplot(castle_dat_test, aes(x = motor, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Motor", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

(p1+p2)/(p3+p4)+ plot_layout(guides = "collect") & theme(legend.position = "bottom")
(p5+p6)/(p7+p8)+ plot_layout(guides = "collect") & theme(legend.position = "bottom")
(p9+p10)/(p11+p12)+ plot_layout(guides = "collect") & theme(legend.position = "bottom")
```

## With splines: {.appendix}

### eCDFS: {.appendix}

```{r, fig.height=8, fig.width = 8, warning = FALSE, echo = FALSE, fig.caption = "eCDFs"}
p1 <- ggplot(castle_dat, aes(x = burglary, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Burglary", color = "Castle Implemented") +
  theme_minimal()

p2 <- ggplot(castle_dat, aes(x = burglary, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Burglary", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p3 <- ggplot(castle_dat, aes(x = assault, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Assault", color = "Castle Implemented") +
  theme_minimal()

p4 <- ggplot(castle_dat, aes(x = assault, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Assault", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p5 <- ggplot(castle_dat, aes(x = robbery, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Robbery", color = "Castle Implemented") +
  theme_minimal()

p6 <- ggplot(castle_dat, aes(x = robbery, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Robbery", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p7 <- ggplot(castle_dat, aes(x = homicide, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Homicide", color = "Castle Implemented") +
  theme_minimal()

p8 <- ggplot(castle_dat, aes(x = homicide, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Homicide", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p9 <- ggplot(castle_dat, aes(x = robbery_gun_r, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Robbery with Gun", color = "Castle Implemented") +
  theme_minimal()

p10 <- ggplot(castle_dat, aes(x = robbery_gun_r, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Robbery with Gun", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

p11 <- ggplot(castle_dat, aes(x = motor, color = factor(post))) + 
  geom_ecdf() +
  theme(legend.position = "bottom") +
  labs(x = "Motor", color = "Castle Implemented") +
  theme_minimal()

p12 <- ggplot(castle_dat, aes(x = motor, color = factor(post))) + 
  geom_ecdf(aes(weights = w_att)) +
  theme(legend.position = "bottom") +
  labs(x = "Motor", color = "Castle Implemented") +
  ggtitle("ATT") +
  theme_minimal()

(p1+p2)/(p3+p4)+ plot_layout(guides = "collect") & theme(legend.position = "bottom")
(p5+p6)/(p7+p8)+ plot_layout(guides = "collect") & theme(legend.position = "bottom")
(p9+p10)/(p11+p12)+ plot_layout(guides = "collect") & theme(legend.position = "bottom")

```

## Alternate Adjustment Set: {.appendix}

```{r, fig.caption = "Mirrored Histograms of Propensity Scores, Alternate"}
p1 <- ggplot(castle_dat_alt, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = .6, fill = "grey") +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post), weight = w_att)) +
  labs(x = "Propensity Score", y = "Count", fill = "Passed Castle Doctrine", caption = "Mirrored Histogram of Propensity Scores") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")

p2 <- ggplot(castle_dat_alt, aes(x = .fitted, group = post, fill = post)) +
  geom_mirror_histogram(bins = 30, alpha = 1, aes(fill = factor(post), weight = w_att)) +
  labs(x = "Propensity Score", y = "Count", fill = "Passed Castle Doctrine", caption = "Mirrored Histogram of Propensity Scores") +
  scale_y_continuous(labels = abs) +
  scale_fill_manual(labels = c("No", "Yes"), values = c("turquoise", "coral")) +
  theme_minimal() +
  theme(legend.position = "bottom")

(p1 + p2) + plot_layout(guides = "collect") & theme(legend.position = "bottom")
```

```{r, fig.caption = "Alternate Love Plot"}
weighted_for_love_alt <- tidy_smd(
  castle_dat_alt,
  .vars = c(poverty, robbery, robbery_gun_r, whitem_15_24, blackm_15_24, whitem_25_44, blackm_25_44),
  .group = post,
  .wts = c(w_att)
)

ggplot(data = weighted_for_love_alt, aes(x = abs(smd), y = variable, group = method, color = method)) +
  geom_love() +
  scale_color_manual(values = c("coral", "turquoise"), labels = c("Oberserved", "ATT Weighted")) + 
  labs(color = "Method", x = "Absolute Value of SMD", y = "Variable") +
  theme_minimal()
```
